var documenterSearchIndex = {"docs":
[{"location":"lib/FileFormats/#FileFormats-module","page":"FileFormats module","title":"FileFormats module","text":"Pages = [\"FileFormats.md\"]\nDepth = 3","category":"section"},{"location":"lib/FileFormats/#Reading-neural-networks","page":"FileFormats module","title":"Reading neural networks","text":"","category":"section"},{"location":"lib/FileFormats/#Writing-neural-networks","page":"FileFormats module","title":"Writing neural networks","text":"","category":"section"},{"location":"lib/FileFormats/#ControllerFormats.FileFormats","page":"FileFormats module","title":"ControllerFormats.FileFormats","text":"FileFormats\n\nModule to parse and write file formats of controllers.\n\n\n\n\n\n","category":"module"},{"location":"lib/FileFormats/#ControllerFormats.FileFormats.read_MAT","page":"FileFormats module","title":"ControllerFormats.FileFormats.read_MAT","text":"read_MAT(filename::String; act_key::String)\n\nRead a neural network stored in MATLAB's MAT format. This function requires to load the MAT.jl library.\n\nInput\n\nfilename – name of the MAT file\nact_key  – key used for the activation functions\nnet_key  – (optional; default: nothing) key used for the neural network\n\nOutput\n\nA FeedforwardNetwork.\n\nNotes\n\nThe MATLAB file encodes a dictionary. If net_key is given, then the dictionary contains another dictionary under this key. Otherwise the outer dictionary directly contains the following:\n\nA vector of weight matrices (under the name \"W\")\nA vector of bias vectors (under the name \"b\")\nA vector of strings for the activation functions (under the name passed via act_key)\n\n\n\n\n\n","category":"function"},{"location":"lib/FileFormats/#ControllerFormats.FileFormats.read_NNet","page":"FileFormats module","title":"ControllerFormats.FileFormats.read_NNet","text":"read_NNet(filename::String)\n\nRead a neural network stored in NNet format.\n\nInput\n\nfilename – name of the NNet file\n\nOutput\n\nA FeedforwardNetwork.\n\nNotes\n\nThe format assumes that all layers but the output layer use ReLU activation (the output layer uses the identity activation).\n\nThe format looks like this (each line may optionally be terminated by a comma):\n\nHeader text, each line beginning with \"//\"\nComma-separated line with four values: number of layer operations, number of inputs, number of outputs, maximum layer size\nComma-separated line with the layer sizes\nFlag that is no longer used\nMinimum values of inputs\nMaximum values of inputs\nMean values of inputs and one value for all outputs\nRange values of inputs and one value for all outputs\nBlocks of lines describing the weight matrix and bias vector for a layer; each matrix row is written as a comma-separated line, and each vector entry is written in its own line\n\nThe code follows this implementation.\n\n\n\n\n\n","category":"function"},{"location":"lib/FileFormats/#ControllerFormats.FileFormats.read_ONNX","page":"FileFormats module","title":"ControllerFormats.FileFormats.read_ONNX","text":"read_ONNX(filename::String; [input_dimension=nothing])\n\nRead a neural network stored in ONNX format. This function requires to load the ONNX.jl library.\n\nInput\n\nfilename        – name of the ONNX file\ninput_dimension – (optional; default: nothing) input dimension (required                      by ONNX.jl parser); see the notes below\n\nOutput\n\nA FeedforwardNetwork.\n\nNotes\n\nThis implementation assumes the following structure:\n\nFirst comes the input vector (which is ignored).\nNext come the weight matrices W (transposed) and bias vectors b in pairs in the order in which they are applied.\nNext come the affine maps and the activation functions in the order in which they are applied. The last layer does not have an activation function.\n\nSome of these assumptions are currently not validated. Hence it may happen that this function returns a result that is incorrect.\n\nIf the argument input_dimension is not provided, the file is parsed an additional time to read the correct number (which is inefficient).\n\n\n\n\n\n","category":"function"},{"location":"lib/FileFormats/#ControllerFormats.FileFormats.read_POLAR","page":"FileFormats module","title":"ControllerFormats.FileFormats.read_POLAR","text":"read_POLAR(filename::String)\n\nRead a neural network stored in POLAR format.\n\nInput\n\nfilename – name of the POLAR file\n\nOutput\n\nA FeedforwardNetwork.\n\nNotes\n\nThe POLAR format uses the same parameter format as Sherlock (see read_Sherlock) but allows for general activation functions.\n\nIn addition, the last two lines are:\n\n0.0\n1.0\n\nThe reference parser and writer can be found here.\n\n\n\n\n\n","category":"function"},{"location":"lib/FileFormats/#ControllerFormats.FileFormats.read_Sherlock","page":"FileFormats module","title":"ControllerFormats.FileFormats.read_Sherlock","text":"read_Sherlock(filename::String)\n\nRead a neural network stored in Sherlock format.\n\nInput\n\nfilename – name of the Sherlock file\n\nOutput\n\nA FeedforwardNetwork.\n\nNotes\n\nAll layers including the output layer implicitly use a ReLU activation function.\n\n\n\n\n\n","category":"function"},{"location":"lib/FileFormats/#ControllerFormats.FileFormats.read_YAML","page":"FileFormats module","title":"ControllerFormats.FileFormats.read_YAML","text":"read_YAML(filename::String)\n\nRead a neural network stored in YAML format. This function requires to load the YAML.jl library.\n\nInput\n\nfilename – name of the YAML file\n\nOutput\n\nA FeedforwardNetwork.\n\n\n\n\n\n","category":"function"},{"location":"lib/FileFormats/#ControllerFormats.FileFormats.write_NNet","page":"FileFormats module","title":"ControllerFormats.FileFormats.write_NNet","text":"write_NNet(N::FeedforwardNetwork, filename::String)\n\nWrite a neural network to a file in NNet format.\n\nInput\n\nN        – feedforward neural network\nfilename – name of the output file\n\nOutput\n\nnothing. The network is written to the output file.\n\nNotes\n\nThe NNet format assumes that all layers but the output layer use ReLU activation (the output layer uses the identity activation).\n\nSome non-important part of the output (such as the input domain) is not correctly written and instead set to 0.\n\nSee read_NNet for the documentation of the format.\n\n\n\n\n\n","category":"function"},{"location":"lib/FileFormats/#ControllerFormats.FileFormats.write_POLAR","page":"FileFormats module","title":"ControllerFormats.FileFormats.write_POLAR","text":"write_POLAR(N::FeedforwardNetwork, filename::String)\n\nWrite a neural network to a file in POLAR format.\n\nInput\n\nN        – feedforward neural network\nfilename – name of the output file\n\nOutput\n\nnothing. The network is written to the output file.\n\n\n\n\n\n","category":"function"},{"location":"lib/FileFormats/#ControllerFormats.FileFormats.write_Sherlock","page":"FileFormats module","title":"ControllerFormats.FileFormats.write_Sherlock","text":"write_Sherlock(N::FeedforwardNetwork, filename::String)\n\nWrite a neural network to a file in Sherlock format.\n\nInput\n\nN        – feedforward neural network\nfilename – name of the output file\n\nOutput\n\nnothing. The network is written to the output file.\n\nNotes\n\nThe Sherlock format requires that all activation functions are ReLU.\n\n\n\n\n\n","category":"function"},{"location":"lib/Architecture/#Architecture-module","page":"Architecture module","title":"Architecture module","text":"Pages = [\"Architecture.md\"]\nDepth = 3","category":"section"},{"location":"lib/Architecture/#Neural-networks","page":"Architecture module","title":"Neural networks","text":"An artificial neural network can be used as a controller.","category":"section"},{"location":"lib/Architecture/#General-interface","page":"Architecture module","title":"General interface","text":"The following non-standard methods are implemented:","category":"section"},{"location":"lib/Architecture/#Implementation","page":"Architecture module","title":"Implementation","text":"","category":"section"},{"location":"lib/Architecture/#Layer-operations","page":"Architecture module","title":"Layer operations","text":"The following non-standard methods are useful to implement:","category":"section"},{"location":"lib/Architecture/#More-specific-layer-interfaces","page":"Architecture module","title":"More specific layer interfaces","text":"","category":"section"},{"location":"lib/Architecture/#Implementation-2","page":"Architecture module","title":"Implementation","text":"","category":"section"},{"location":"lib/Architecture/#Activation-functions","page":"Architecture module","title":"Activation functions","text":"The following strings can be parsed as activation functions:\n\nusing ControllerFormats  # hide\nControllerFormats.FileFormats.available_activations","category":"section"},{"location":"lib/Architecture/#ControllerFormats.Architecture","page":"Architecture module","title":"ControllerFormats.Architecture","text":"Architecture\n\nModule containing data structures to represent controllers.\n\n\n\n\n\n","category":"module"},{"location":"lib/Architecture/#ControllerFormats.Architecture.AbstractNeuralNetwork","page":"Architecture module","title":"ControllerFormats.Architecture.AbstractNeuralNetwork","text":"AbstractNeuralNetwork\n\nAbstract type for neural networks.\n\nNotes\n\nSubtypes should implement the following method:\n\nlayers(::AbstractNeuralNetwork) - return a list of the layers\n\nThe following standard methods are implemented:\n\nlength(::AbstractNeuralNetwork)\ngetindex(::AbstractNeuralNetwork, indices)\nlastindex(::AbstractNeuralNetwork)\n==(::AbstractNeuralNetwork, ::AbstractNeuralNetwork)\n\n\n\n\n\n","category":"type"},{"location":"lib/Architecture/#ControllerFormats.Architecture.layers-Tuple{AbstractNeuralNetwork}","page":"Architecture module","title":"ControllerFormats.Architecture.layers","text":"layers(N::AbstractNeuralNetwork)\n\nReturn a list of the layers of a neural network.\n\nInput\n\nN – neural network\n\nOutput\n\nThe list of layers.\n\n\n\n\n\n","category":"method"},{"location":"lib/Architecture/#ControllerFormats.Architecture.dim_in-Tuple{AbstractNeuralNetwork}","page":"Architecture module","title":"ControllerFormats.Architecture.dim_in","text":"dim_in(N::AbstractNeuralNetwork)\n\nReturn the input dimension of a neural network.\n\nInput\n\nN – neural network\n\nOutput\n\nThe dimension of the input layer of N.\n\n\n\n\n\n","category":"method"},{"location":"lib/Architecture/#ControllerFormats.Architecture.dim_out-Tuple{AbstractNeuralNetwork}","page":"Architecture module","title":"ControllerFormats.Architecture.dim_out","text":"dim_out(N::AbstractNeuralNetwork)\n\nReturn the output dimension of a neural network.\n\nInput\n\nN – neural network\n\nOutput\n\nThe dimension of the output layer of N.\n\n\n\n\n\n","category":"method"},{"location":"lib/Architecture/#ControllerFormats.Architecture.dim-Tuple{AbstractNeuralNetwork}","page":"Architecture module","title":"ControllerFormats.Architecture.dim","text":"dim(N::AbstractNeuralNetwork)\n\nReturn the input and output dimension of a neural network.\n\nInput\n\nN – neural network\n\nOutput\n\nThe pair (i o) where i is the input dimension and o is the output dimension of N.\n\nNotes\n\nThis function is not exported due to name conflicts with other related packages.\n\n\n\n\n\n","category":"method"},{"location":"lib/Architecture/#ControllerFormats.Architecture.FeedforwardNetwork","page":"Architecture module","title":"ControllerFormats.Architecture.FeedforwardNetwork","text":"FeedforwardNetwork{L} <: AbstractNeuralNetwork\n\nStandard implementation of a feedforward neural network which stores the layer operations.\n\nFields\n\nlayers – vector of layer operations (see AbstractLayerOp)\n\nNotes\n\nThe field layers contains the layer operations, so the number of layers is length(layers) + 1.\n\nConversion from a Flux.Chain is supported.\n\n\n\n\n\n","category":"type"},{"location":"lib/Architecture/#ControllerFormats.Architecture.AbstractLayerOp","page":"Architecture module","title":"ControllerFormats.Architecture.AbstractLayerOp","text":"AbstractLayerOp\n\nAbstract type for layer operations.\n\nNotes\n\nAn AbstractLayerOp represents a layer operation. A classical example is a \"dense layer operation\" with an affine map followed by an activation function.\n\n\n\n\n\n","category":"type"},{"location":"lib/Architecture/#ControllerFormats.Architecture.dim_in-Tuple{AbstractLayerOp}","page":"Architecture module","title":"ControllerFormats.Architecture.dim_in","text":"dim_in(L::AbstractLayerOp)\n\nReturn the input dimension of a layer operation.\n\nInput\n\nL – layer operation\n\nOutput\n\nThe input dimension of L.\n\n\n\n\n\n","category":"method"},{"location":"lib/Architecture/#ControllerFormats.Architecture.dim_out-Tuple{AbstractLayerOp}","page":"Architecture module","title":"ControllerFormats.Architecture.dim_out","text":"dim_out(L::AbstractLayerOp)\n\nReturn the output dimension of a layer operation.\n\nInput\n\nL – layer operation\n\nOutput\n\nThe output dimension of L.\n\n\n\n\n\n","category":"method"},{"location":"lib/Architecture/#ControllerFormats.Architecture.dim-Tuple{AbstractLayerOp}","page":"Architecture module","title":"ControllerFormats.Architecture.dim","text":"dim(L::AbstractLayerOp)\n\nReturn the input and output dimension of a layer operation.\n\nInput\n\nN – neural network\n\nOutput\n\nThe pair (i o) where i is the input dimension and o is the output dimension of N.\n\nNotes\n\nThis function is not exported due to name conflicts with other related packages.\n\n\n\n\n\n","category":"method"},{"location":"lib/Architecture/#ControllerFormats.Architecture.AbstractPoolingLayerOp","page":"Architecture module","title":"ControllerFormats.Architecture.AbstractPoolingLayerOp","text":"AbstractPoolingLayerOp <: AbstractLayerOp\n\nAbstract type for pooling layer operations.\n\nNotes\n\nPooling is an operation on a three-dimensional tensor that iterates over the first two dimensions in a window and aggregates the values, thus reducing the output dimension.\n\nImplementation\n\nThe following (unexported) functions should be implemented:\n\nwindow(::AbstractPoolingLayerOp)   – return the pair (p q) representing the window size\naggregation(::AbstractPoolingLayerOp) – return the aggregation function (applied to a tensor)\n\n\n\n\n\n","category":"type"},{"location":"lib/Architecture/#ControllerFormats.Architecture.DenseLayerOp","page":"Architecture module","title":"ControllerFormats.Architecture.DenseLayerOp","text":"DenseLayerOp{F, M, B} <: AbstractLayerOp\n\nA dense layer operation is an affine map followed by an activation function.\n\nFields\n\nweights    – weight matrix\nbias       – bias vector\nactivation – activation function\n\nNotes\n\nConversion from a Flux.Dense is supported.\n\n\n\n\n\n","category":"type"},{"location":"lib/Architecture/#ControllerFormats.Architecture.ConvolutionalLayerOp","page":"Architecture module","title":"ControllerFormats.Architecture.ConvolutionalLayerOp","text":"ConvolutionalLayerOp{F, M, B} <: AbstractLayerOp\n\nA convolutional layer operation is a series of filters, each of which computes a small affine map followed by an activation function.\n\nFields\n\nweights    – vector with one weight matrix for each filter\nbias       – vector with one bias value for each filter\nactivation – activation function\n\nNotes\n\nConversion from a Flux.Conv is supported.\n\n\n\n\n\n","category":"type"},{"location":"lib/Architecture/#ControllerFormats.Architecture.FlattenLayerOp","page":"Architecture module","title":"ControllerFormats.Architecture.FlattenLayerOp","text":"FlattenLayerOp <: AbstractLayerOp\n\nA flattening layer operation converts a multidimensional tensor into a vector.\n\nNotes\n\nThe implementation uses row-major ordering for convenience with the machine-learning literature.\n\njulia> T = reshape([1, 3, 2, 4, 5, 7, 6, 8], (2, 2, 2))\n2×2×2 Array{Int64, 3}:\n[:, :, 1] =\n 1  2\n 3  4\n\n[:, :, 2] =\n 5  6\n 7  8\n\njulia> FlattenLayerOp()(T)\n8-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n\n\n\n\n\n","category":"type"},{"location":"lib/Architecture/#ControllerFormats.Architecture.MaxPoolingLayerOp","page":"Architecture module","title":"ControllerFormats.Architecture.MaxPoolingLayerOp","text":"MaxPoolingLayerOp <: AbstractPoolingLayerOp\n\nA max-pooling layer operation. The aggregation function is maximum.\n\nFields\n\np – horizontal window size\nq – vertical window size\n\n\n\n\n\n","category":"type"},{"location":"lib/Architecture/#ControllerFormats.Architecture.MeanPoolingLayerOp","page":"Architecture module","title":"ControllerFormats.Architecture.MeanPoolingLayerOp","text":"MeanPoolingLayerOp <: AbstractPoolingLayerOp\n\nA mean-pooling layer operation. The aggregation function is Statistics.mean.\n\nFields\n\np – horizontal window size\nq – vertical window size\n\n\n\n\n\n","category":"type"},{"location":"lib/Architecture/#ControllerFormats.Architecture.ActivationFunction","page":"Architecture module","title":"ControllerFormats.Architecture.ActivationFunction","text":"ActivationFunction\n\nAbstract type for activation functions.\n\n\n\n\n\n","category":"type"},{"location":"lib/Architecture/#ControllerFormats.Architecture.Id","page":"Architecture module","title":"ControllerFormats.Architecture.Id","text":"Id\n\nIdentity activation.\n\n    f(x) = x\n\n\n\n\n\n","category":"type"},{"location":"lib/Architecture/#ControllerFormats.Architecture.ReLU","page":"Architecture module","title":"ControllerFormats.Architecture.ReLU","text":"ReLU\n\nRectified linear unit (ReLU) activation.\n\n    f(x) = max(x 0)\n\n\n\n\n\n","category":"type"},{"location":"lib/Architecture/#ControllerFormats.Architecture.Sigmoid","page":"Architecture module","title":"ControllerFormats.Architecture.Sigmoid","text":"Sigmoid\n\nSigmoid activation.\n\n    f(x) = frac11 + e^-x\n\n\n\n\n\n","category":"type"},{"location":"lib/Architecture/#ControllerFormats.Architecture.Tanh","page":"Architecture module","title":"ControllerFormats.Architecture.Tanh","text":"Tanh\n\nHyperbolic tangent activation.\n\n    f(x) = tanh(x) = frace^x - e^-xe^x + e^-x\n\n\n\n\n\n","category":"type"},{"location":"lib/Architecture/#ControllerFormats.Architecture.LeakyReLU","page":"Architecture module","title":"ControllerFormats.Architecture.LeakyReLU","text":"LeakyReLU{N<:Number}\n\nLeaky ReLU activation.\n\n    fₐ(x) = x  0  x  a x\n\nwhere a is the parameter.\n\nFields\n\nslope – parameter for negative inputs\n\n\n\n\n\n","category":"type"},{"location":"about/#About","page":"About","title":"About","text":"This page contains some general information about this project and recommendations about contributing.","category":"section"},{"location":"about/#Contributing","page":"About","title":"Contributing","text":"If you like this package, consider contributing!\n\nCreating an issue in the ControllerFormats GitHub issue tracker to report a bug, open a discussion about existing functionality, or suggest new functionality is appreciated.\n\nIf you have written code and would like it to be peer reviewed and added to the library, you can fork the repository and send a pull request (see below).\n\nYou are also welcome to get in touch with us in the JuliaReach Zulip channel.\n\nBelow we give some general comments about contributing to this package. The JuliaReach development documentation describes coding guidelines; take a look when in doubt about the coding style that is expected for the code that is finally merged into the library.","category":"section"},{"location":"about/#Branches-and-pull-requests-(PR)","page":"About","title":"Branches and pull requests (PR)","text":"We use a standard pull-request policy: You work in a private branch and eventually add a pull request, which is then reviewed by other programmers and merged into the master branch.\n\nEach pull request should be based on a branch with the name of the author followed by a descriptive name, e.g., mforets/my_feature. If the branch is associated to a previous discussion in an issue, we use the number of the issue for easier lookup, e.g., mforets/7.","category":"section"},{"location":"about/#Unit-testing-and-continuous-integration-(CI)","page":"About","title":"Unit testing and continuous integration (CI)","text":"This project is synchronized with GitHub Actions such that each PR gets tested before merging (and the build is automatically triggered after each new commit). For the maintainability of this project, it is important to make all unit tests pass.\n\nTo run the unit tests locally, you can do:\n\njulia> using Pkg\n\njulia> Pkg.test(\"ControllerFormats\")\n\nWe also advise adding new unit tests when adding new features to ensure long-term support of your contributions.","category":"section"},{"location":"about/#Contributing-to-the-documentation","page":"About","title":"Contributing to the documentation","text":"New functions and types should be documented according to the JuliaReach development documentation.\n\nYou can view the source-code documentation from inside the REPL by typing ? followed by the name of the type or function.\n\nThe documentation you are currently reading is written in Markdown, and it relies on the package Documenter.jl to produce the final layout. The sources for creating this documentation are found in docs/src. You can easily include the documentation that you wrote for your functions or types there (see the source code or Documenter's guide for examples).\n\nTo generate the documentation locally, run docs/make.jl, e.g., by executing the following command in the terminal:\n\n$ julia --color=yes docs/make.jl","category":"section"},{"location":"about/#Credits","page":"About","title":"Credits","text":"Here we list the names of the maintainers of the ControllerFormats.jl library, as well as past and present contributors (in alphabetic order).","category":"section"},{"location":"about/#Core-developers","page":"About","title":"Core developers","text":"Marcelo Forets, Universidad de la República\nChristian Schilling, Aalborg University","category":"section"},{"location":"about/#Contributors","page":"About","title":"Contributors","text":"Sebastián Guadalupe, Universidad de la República","category":"section"},{"location":"lib/ControllerFormats/#ControllerFormats.jl","page":"ControllerFormats module","title":"ControllerFormats.jl","text":"","category":"section"},{"location":"lib/ControllerFormats/#ControllerFormats.ControllerFormats","page":"ControllerFormats module","title":"ControllerFormats.ControllerFormats","text":"ControllerFormats\n\nModule for representations of controllers.\n\nSubmodules\n\nArchitecture – data structures for controllers\nFileFormats  – IO of file representations of controllers\n\n\n\n\n\n","category":"module"},{"location":"#ControllerFormats.jl","page":"Home","title":"ControllerFormats.jl","text":"This light-weight Julia library contains basic representations of controllers (currently deep neural networks) as well as functionality to parse them from various file formats like MAT, YAML and ONNX.\n\nThe library originated from the package ClosedLoopReachability, which performs formal analysis of a given trained neural network. This motivates that ControllerFormats.jl does not provide support for typical other tasks such as network training, and some of the supported file formats are only used by some similar analysis tool.","category":"section"},{"location":"#Related-packages","page":"Home","title":"Related packages","text":"Flux.jl is a comprehensive Julia framework for machine learning. It also offers a representation of neural networks.\nMLJ.jl is a large Julia library of machine-learning models such as neural networks and decision trees.\nNNet offers a representation of neural networks and a parser for the NNet format.","category":"section"}]
}
